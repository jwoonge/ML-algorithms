{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKQRlFSRHvncCh/pGHH1sW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwoonge/ML-algorithms/blob/master/10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkpKaXAz2twv",
        "colab_type": "code",
        "outputId": "5b477bcc-72cf-418b-bb0b-1b62f9bd23c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "!curl https://colab.chainer.org/install | sh -"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   7314      0 --:--:-- --:--:-- --:--:--  7281\r100  1580  100  1580    0     0   7281      0 --:--:-- --:--:-- --:--:--  7247\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAENtEJ264fk",
        "colab_type": "code",
        "outputId": "8c7000cf-1c78-4a95-8102-21a646554d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.15.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG_vBCoS_9Oe",
        "colab_type": "code",
        "outputId": "a6add7d1-bf69-4128-dff9-a6d1224a1860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cupy as cp\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6V2hrEMEnGQ",
        "colab_type": "code",
        "outputId": "cd0604c7-0c5d-4546-9e1d-31caa646c078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"movie_review.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2MSpVuu6E0u",
        "colab_type": "code",
        "outputId": "41d40413-f948-4c8c-f381-e108398f2b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#'My_Drive/Colab_Notebooks/ML_assignments/10'\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "X_trian = cp.array(X_train) ; X_test = cp.array(X_test)\n",
        "y_train = cp.array(y_train) ; y_test = cp.array(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bl6dIHyFf7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(input):\n",
        "    return 1/(1+cp.exp(-input))\n",
        "def sigmoid_d(input):\n",
        "    return sigmoid(input)*(1-sigmoid(input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TXYTcUFkjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(h, label, weights, lam):\n",
        "    pre = cp.sum(cp.average(-label * cp.log(h) - (1-label)*cp.log(1-h), axis=0))\n",
        "    n = 0\n",
        "    theta_sum = 0\n",
        "\n",
        "    for weight in weights:\n",
        "        n += weight.size\n",
        "        theta_sum += cp.sum(cp.square(weight))\n",
        "    reg = lam/(2*n)*theta_sum\n",
        "\n",
        "    return pre + reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K47rKH67FmSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(h, label):\n",
        "    predicted = cp.where(h[:,0]>0.5, 1,0)\n",
        "    correct = predicted[predicted==label]\n",
        "    return len(correct)/len(predicted)*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfBKX5sDFnnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class classifier:\n",
        "    def __init__(self, shape, lam, learning_rate):\n",
        "        self.weights = []\n",
        "        for i in range(len(shape)-1):\n",
        "            self.weights.append(cp.random.randn(shape[i]+1, shape[i+1]))\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lam = lam\n",
        "        self.num_layer = len(self.weights)\n",
        "        self.n = 0\n",
        "        for weight in self.weights:\n",
        "            self.n += weight.size\n",
        "    \n",
        "    def forward_pass(self, input, weights=[]):\n",
        "        if weights==[]:\n",
        "            weights = self.weights\n",
        "        values = []\n",
        "        values.append(cp.array(np.insert(cp.asnumpy(input), 0, 1, axis=1)))\n",
        "        values.append(cp.dot(values[-1], weights[0]))\n",
        "        values.append(cp.array(np.insert(cp.asnumpy(sigmoid(values[-1])),0,1,axis=1)))\n",
        "        for i in range(1, len(weights)-1):\n",
        "            values.append(cp.dot(values[-1], weights[i]))\n",
        "            values.append(cp.array(np.insert(cp.asnumpy(sigmoid(values[-1])),0,1,axis=1)))\n",
        "        values.append(cp.dot(values[-1], weights[-1]))\n",
        "        values.append(sigmoid(values[-1]))\n",
        "        return values\n",
        "\n",
        "    def gradient_descent(self, forward_values, label):\n",
        "        update_values = []\n",
        "        m = len(label)\n",
        "        mult = (sigmoid_d(forward_values[-2]) * ((1-label)/(1-forward_values[-1]) - label/forward_values[-1])).T\n",
        "        layer = len(self.weights)-1\n",
        "        update_values.append(cp.dot(mult, forward_values[2*layer]).T)\n",
        "        for i in range(1, len(self.weights)):\n",
        "            layer = len(self.weights) - i - 1\n",
        "            mult = (cp.dot(mult.T, self.weights[layer+1][1:,:].T) * (sigmoid_d(forward_values[layer*2+1]))).T\n",
        "            update_values.append(cp.dot(mult, forward_values[2*layer]).T)\n",
        "        for i in range(len(self.weights)):\n",
        "            layer = len(self.weights)-i-1\n",
        "            self.weights[layer] = self.weights[layer] - self.learning_rate * (update_values[i]/m + self.lam*self.weights[layer]/self.n)\n",
        "\n",
        "    def optimize(self, data_train, label_train, data_test, label_test, epoch):\n",
        "        forward_train = self.forward_pass(data_train)\n",
        "        forward_test = self.forward_pass(data_test)\n",
        "        h_train = forward_train[-1]\n",
        "        h_test = forward_test[-1]\n",
        "        loss_train = [loss(h_train, label_train, self.weights, self.lam)]\n",
        "        loss_test = [loss(h_test, label_test, self.weights, self.lam)]\n",
        "        accuracy_train = [accuracy(h_train, label_train)]\n",
        "        accuracy_test = [accuracy(h_test, label_test)]\n",
        "        self.best_train_accuracy = accuracy_train[-1]\n",
        "        self.best_test_accuracy = accuracy_test[-1]\n",
        "        self.best_train_loss = loss_train[-1]\n",
        "        self.best_test_loss = loss_test[-1]\n",
        "        self.best_weights = self.weights\n",
        "        \n",
        "        for i in range(epoch-1):\n",
        "\n",
        "            self.gradient_descent(forward_train, label_train)\n",
        "            forward_train = self.forward_pass(data_train)\n",
        "            forward_test = self.forward_pass(data_test)\n",
        "            h_train = forward_train[-1]\n",
        "            h_test = forward_test[-1]\n",
        "            loss_train.append(loss(h_train, label_train, self.weights, self.lam))\n",
        "            loss_test.append(loss(h_test, label_test, self.weights, self.lam))\n",
        "            accuracy_train.append(accuracy(h_train, label_train))\n",
        "            accuracy_test.append(accuracy(h_test, label_test))\n",
        "            \n",
        "            if loss_test[-1] < self.best_test_loss:\n",
        "                self.best_test_loss = loss_test[-1]\n",
        "                self.best_train_loss = loss_train[-1]\n",
        "                self.best_test_accuracy = accuracy_test[-1]\n",
        "                self.best_train_accuracy = accuracy_train[-1]\n",
        "                self.best_weights = self.weights\n",
        "\n",
        "            print(i,\" LOS_T:\", format(float(loss_train[-1]),'.5f'), \"\\tACC_T:\", format(float(accuracy_train[-1]),'.5f'), \"| LOS_V:\", format(float(loss_test[-1]),'.5f'), \"ACC_V:\" ,format(float(accuracy_test[-1]),'.5f'), format(self.lam/self.n,'.10f'), self.n)\n",
        "        return loss_train, accuracy_train, loss_test, accuracy_test\n",
        "    \n",
        "    def predict(self, datas):\n",
        "        return self.forward_pass(datas, weights=self.best_weights)[-1]\n",
        "    def predict_class(self, datas, best=True):\n",
        "        if best:\n",
        "            weights = self.best_weights\n",
        "        else:\n",
        "            weights = self.weights\n",
        "        return cp.argmax(self.forward_pass(datas, weights=weights)[-1], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2_kDnufF8N2",
        "colab_type": "code",
        "outputId": "8a9d1a91-cddb-4e12-ea2d-6b7d23f7d3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "Classifier = classifier([1500, 56, 24, 1], lam=27, learning_rate=1.5)\n",
        "loss_train, accuracy_train, loss_test, accuracy_test = Classifier.optimize(X_train, cp.reshape(y_train,[len(y_train),1]), X_test, y_test, 200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4e58637fd917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-8bf855abea70>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, data_train, label_train, data_test, label_test, epoch)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mforward_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mforward_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8bf855abea70>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(self, forward_values, label)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mmult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mupdate_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/linalg/product.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# TODO(okuta): check type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.dot\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.dot\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Axis dimension mismatch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYPV5YHTJpVN",
        "colab_type": "code",
        "outputId": "195e283d-eedc-46a9-fbd2-7094ec1ba714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "print(np.shape(X_train))\n",
        "print(len(y_train))\n",
        "y_train = cp.reshape(y_train, (len(y_train),1))\n",
        "print(np.shape(y_train))\n",
        "#correct = h[np.abs(우리꺼 - 정답)<0.5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1401, 1500)\n",
            "1401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2f8f5c92270c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#correct = h[np.abs(우리꺼 - 정답)<0.5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: no implementation found for 'numpy.shape' on types that implement __array_function__: [<class 'cupy.core.core.ndarray'>]"
          ]
        }
      ]
    }
  ]
}